---
title: "Spam SMS classifier with Naive Bays"
author: "Mohammed Zakaria"
#output:
#  html_document:
#    df_print: paged
#    toc: yes
#   keep_md: true
#    always_allow_html: yes

  
#  html_notebook:
#    highlight: espresso
#    theme: readable
#    toc: yes
always_allow_html: yes # this is for wordcloud2 output
output: github_document

---



Required packages
```{r}
#install.packages("class") # for kNN classification
#library(class)
#install.packages("gmodels") # for CrossTable function at the evaluation
library(gmodels)
#install.packages("caret") # for model tuning
library(caret)
#install.packages("e1071") # to give us Naive Bayes 
library(e1071)
#install.packages("pROC") # to make ROC plots
library(pROC)	
#install.packages("tm") # to handle text data
library(tm)
#install.packages("SnowballC") # for steaming
library(SnowballC)
#install.packages("wordcloud2") # to create word cloud
library(wordcloud2)
#install.packages("widgetframe")
#library(widgetframe)
#devtools::install_github('ramnathv/htmlwidgets')
library(wordcloud)
```


Pulling the data: The cancer data is from Brett Lantz's "Machine Learning with R"
a repo for the data is under this link:
https://github.com/mzakariaCERN/Machine-Learning-with-R-datasets/blob/master/wisc_bc_data.csv
and original data can be found under
https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/
```{r}
sms_raw <- read.csv(file="C:/Users/mkzak/Documents/GitHub/FunWithR/FunWithR/2_NB/Data/sms_spam.csv", stringsAsFactors = FALSE)


dim(sms_raw)
str(sms_raw)
summary(sms_raw)

```

We see there are two features. And the feature type has a categorical variables. So we need to convert it to factor


```{r}
sms_raw$type <- as.factor(sms_raw$type)

str(sms_raw$type)
table(sms_raw$type)
```

After installing (loading) tm library, we need to create a container for all the text we are dealing with. This is called a corpus. WE will use 
VCorpus (for volatile corpus: corpus stored in memove, compare it to PCorpus, which is stored on disk).


```{r}
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
typeof(sms_corpus)
```


corpus can read from pdf of MS word using readerControl parameter. Check it out! Notice also that corpus is a list objects. So we can manipulate it as such. 

```{r}
print(sms_corpus)
inspect(sms_corpus[1:3])
```


To see one message, we need to grab that element in from the list, and conver it to characters
```{r}
as.character(sms_corpus[[1]])

```

This can be generalized using lapply

```{r}
lapply(sms_corpus[1:2], as.character)
```


Next, we need to clean the text from special characters, capital letters etc, and convert it into separate words. 

First, convert to lower case

```{r}
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
```

remember the first message starts with an upper case letter. Let us take a look now

```{r}
as.character(sms_corpus_clean[[1]])
```

next thing, is to remove numbers from SMS. Though some might be useful for the sender/receiver. It doesn't play much value in spam/ham classification.

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers) # no need for content_tranformer b/c removeNumbers is built in tm. to see other
# built in functions type getTransformations()
```


Next we remove filler (stop) words suc as: 'and', 'or', and 'but'

```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
```

Notice that stoprwords returns a vector of the words we need to consider as such. We can thus add a remove words from the list to modify the
selection

Next, we remove any punctuation


```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)

```

Notice that this function will join words that have punctuation marks between them without any space. Ex: "Hi...Hi" will become "HiHi"
To fix this we can create a function like

```{r}
replacePunctuation <- function(x){gsub("[[:punct:]]", " ", x)}
```
the above function can be used with tm_map

Next, we return all words to their root (AKA: steaming). To do this install and/or load SnowballC


```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
```


next, to remove any extra white space
```{r}
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
```

Finally, we need to convert the messages into word (AKA tokenization). We will convert the corpus into Document Term Matrix: rows are SMS 
messages, and columns are word. Think of it as creating dummy variables for each word.

```{r}
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

sms_dtm
```

Notice that we could have create a DTM from the raw corpus with all the pre-processing in one command
```{r}
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE, removeNumbers = TRUE, stopwords = TRUE, removePunctuation = TRUE, steaming = TRUE))

sms_dtm2
```

notice there are minor differences between the two matrices. this is mainly due to cleaning the corpus after spliting it into words in the second 
option. it also uses a slightly different choice for stop words. To fortce it to use the same stopwords:

```{r}
stopWords = function(x){removeWords(x, stopwords())}
```
and replace stopwrods = TRUE with that function

Next step is splitting the data into training and testing. Since the data is randomized as is, no need to randomize it

```{r}
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170: 5574, ]

sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170: 5574, ]$type
```


Checking that ratio of spam to ham is close in both samples
```{r}
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```


create a word cloud. Lantz uses package wordcount. I will replicate that here and use another package: wordcloud2 that gives js plots with interactive elements


```{r}
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
```

Let us do a cloud for only spam and ham

```{r}
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type = "ham")
wordcloud(spam$text, max.words = 40, scale = c(3, .5))
wordcloud(ham$text, max.words= 40, scale= c(3, .5))
```




Next I will use wordcount2. Which needs a data frame with 
word and frequency. which can be prepared from a TermDocumentMatrix (TDM, and not DTM)

```{r}
myTdm <- as.matrix(TermDocumentMatrix(sms_corpus, control = list(tolower = TRUE, removeNumbers = TRUE, stopwords = TRUE, removePunctuation = TRUE, steaming = TRUE)))
FreqMat <- data.frame(ST = rownames(myTdm),
Freq = rowSums(myTdm),
row.names = NULL)


wc2 <- wordcloud2(FreqMat, minSize = 50)
wc2

```

We can also do similar plots with ham and spam messages to compare between them. Notice that you need to set always_allow_html: yes
in yaml and you can only see the interactive image in html. so it was difficult to deploy on github. 


Since the TDM has one feature per word, we have over 6000 features. Many of these features for words that were mentioned once or twice. These are unlikely to be useful in the classification. so we set a frequency filter to remove any word mentioned less than 5 times.

```{r}
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
str(sms_freq_words)
```

remove every column that matches this vector
```{r}
sms_dtm_freq_train <- sms_dtm_train[, sms_freq_words]
sms_dtm_freq_test  <- sms_dtm_test[,  sms_freq_words]
```

NB classifiers typically use categorical features. This poses an issue for sparce matrices (DTM) since the cell are numeric and measure the number of times a word appears in the same massage. To change it to categorical we convert it to yes/no

```{r}
convert_counts <- function(x){
  x <- ifelse(x > 0, "yes", "no")
}
```


we apply the function on the columns
```{r}
sms_train <- apply(sms_dtm_freq_train, 2, convert_counts)
sms_test <-  apply(sms_dtm_freq_test, 2, convert_counts)
```

