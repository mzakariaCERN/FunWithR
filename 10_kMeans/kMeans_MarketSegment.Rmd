---
title: "Teen Market Segments using k Means"
author: "Mohammed Zakaria"

output: github_document
---



This dataset was compiled by Brett Lantzz (Ch9, Machine Learning with R)

```{r Reading Data}
teens <- read.csv(file = "C:/Users/mkzak/Documents/GitHub/FunWithR/FunWithR/10_kMeans/Data/snsdata.csv")
str(teens)
```

There is a problem with the gender features: we  have NA

```{r The gender feature}
table(teens$gender) # doesn't show it!
table(teens$gender, useNA = "ifany")
summary(teens$gender)
```

```{r the age feature}
summary(teens$age)
```

Also of a concern, anre the min and maximum values. This also needs some cleaning up. A more reasonable range for the age for high school students would be between 13 and 20 years old. Any age value outside of this range should be treated as missing data as cannot trust it. 


```{r standardise age}
teens$age <- ifelse(teens$age >= 13 & teens$age < 20, teens$age, NA)
summary(teens$age)
```

One approach to handle themissing values with a categorical variable - like age - is by assigning it to it's own category. So we can have Male, Female, and dummy code (unknown) as a third gender. 

```{r Dummy coding gender}
teens$female <- ifelse(teens$gender == "F" & !is.na(teens$gender), 1, 0)
teens$no_gender <- ifelse(is.na(teens$gender), 1, 0)
```


Check out the work
```{r verifying dummy coding}
table(teens$gender, useNA = "ifany")

table(teens$female, useNA = "ifany")

table(teens$no_gender, useNA = "ifany")

```

For the 5k some values that are NA. we need to advise a different strategy to infer a better age estimate. We can use graduation year to obtain the value for age

```{r imputation for age}
# to do it for one year
mean(teens$age, na.rm = TRUE)

#to do it for the 4 years
aggregate(data = teens, age ~ gradyear, mean, na.rm = TRUE)
```

For a nice discussion on using aggregate vs the apply family please refer to: https://stackoverflow.com/questions/3505701/grouping-functions-tapply-by-aggregate-and-the-apply-family

The fact that the outout of aggregate is a data frame can cause trouble, so we define our own function:

```{r}
ave_age <- ave(teens$age, teens$gradyear, FUN = function(x) mean(x, na.rm = TRUE))
```

```{r add imputed values to age}
teens$age <- ifelse(is.na(teens$age), ave_age, teens$age)
```


For model building we will use kmeans from the stats package
```{r}
library(stats)
```

The kmeans function requires all numeric features and they should be standardized. Lets start with the 36 features represening interestss

```{r}
interests <- teens[5:40]
```

```{r}
interests_z <- as.data.frame(lapply(interests, scale))
```


We will try clustering with 5 centers:

```{r}
set.seed(2345)
teen_clusters <- kmeans(interests_z, 5)
```


Evaluating how useful the clustering output is somewhat subjectives. It depends on wether these clusters ended up being useful or not. One way to evaluate the utility of a set of clusters is to see how many examples fall in each group. Too large, or too small, groups make them less usefull. 

```{r find size of clusters}
teen_clusters$size
```

Here we notice big difference between the largest and smallest cluster. But on it's own, this might not show any issues. Next we look at cluster cenrers

```{r}
teen_clusters$centers
```
Examining the clusters for any tendencies shows that cluster 3 has above average (that is, positive) for sports. We notice also that cluster 5 lacks any determining domain of interest. 


We add clusters of the kmeans as a feature

```{r}
teens$cluster <- teen_clusters$cluster
```


using aggregate, we can look at the demographic charactaristics of the clusters.

```{r}
aggregate(data = teens, age ~ cluster, mean)
```

We see a more interesting patter regardgin gender

```{r}
aggregate(data = teens, female ~ cluster, mean)
```

```{r}
aggregate(data = teens, friends ~ cluster, mean)
```

The association amoung group membership, gender, and number of friends suggests that the clusters can be useful predictors of behaviour. 

Extra references:   
1. https://www.datasciencecentral.com/profiles/blogs/13-great-articles-about-k-nearest-neighbors-and-related-algorithm  
2. https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering  
3. https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68  