---
title: "Bank Loan classifier with Decision Trees"
author: "Mohammed Zakaria"
#output:
#  html_document:
#    df_print: paged
#    toc: yes
#   keep_md: true
#    always_allow_html: yes

  
#  html_notebook:
#    highlight: espresso
#    theme: readable
#    toc: yes
always_allow_html: yes # this is for wordcloud2 output
output: github_document
---



```{r libraries, message=FALSE, warning= TRUE}
library(gmodels) # for CrossTable
library(ggplot2)
#install.packages("C50") # for DT algorithm
library(C50)
library(caret)
#install.packages('pROC')
library(pROC)
library(dplyr)
```


Pulling the data: The credit data is from Brett Lantz's "Machine Learning with R" a repo for the data is under this link: https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv and original data can be found under https://archive.ics.uci.edu/ml

```{r getting data}
credit <- read.csv(file="C:/Users/mkzak/Documents/GitHub/FunWithR/FunWithR/3_DT/Data/credit.csv", stringsAsFactors = FALSE)
str(credit)

summary(credit)
```

from str() we see that the target feature is actually numerical representing a categorical variable (default vs. no default) and we see that it has 1 for no default and 2 for default. we will label them to make it more readable
```{r convert target feature into factor}
credit$default <- factor(credit$default, labels = c('No', 'YES'))


```


```{r checking few interesting features}
table(credit$checking_balance)

table(credit$default)

```

We divide the data 90:10. WE cannot assume that the data is random. So let us do that

```{r Randomize sample}
set.seed(123)
train_sample <- sample(1000, 900) # get 900 randomly selected numbers, each between 0 and 1000
str(train_sample)
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample,]
```


```{r check if the target feature is equally represented}
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))

```

Close! So we can proceed.

```{r Train model with default parameters}
# remove the "default" feature since this is the target one
credit_model <- C5.0(credit_train[-21], credit_train$default)
credit_model
#summary(credit_model) # uncomment to see summary for all the trees used
```

Here, number of samples is the number of examples
number of predictors is the number of features used
tree size is how many decision the depth of the tree is

More details can be seen from the summary function
```{r}
summary(credit_model)
#plot(credit_model)
```

we understand a line like
checking_balance in {unknown,> 200 DM}: 1 (412/50)
by saying that if we checking balance was unknown, or larger than 200 DM, then we are in class one. (we have 412 examples that we got right, and 50 that we classified wrongly based on this rule)

```{r Predict}
credit_predict <- predict(credit_model, credit_test)
```


```{r}
CrossTable(credit_test$default, credit_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column
```

From the table we can calculate the accuracy as .6 + .14 = 0.74. The model was particularly bad in missing 0.19 of the cases where there was a default. We can try improving the model using boosting

```{r modeling using boosting}
credit_boost10 <- C5.0(credit_train[-21], credit_train$default, trials = 10)
credit_boost10
```
Notice how the average tree size has schrunk! 

```{r}
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column
```

slight improvement accuracy is now 76% and the error rate is %17

Another approcah is to make one type of mistakes costier than the other
```{r Create a cost mistake}
matrix_dimentions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimentions) <- c("predicted", "actual")
matrix_dimentions


```

Assuming that a loan default cost us 4 times as a missed opportunity. We will set the error matrix as the following
```{r setting cost matrix values}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimentions) ## causes problem (trees won't grow), had to do it without dimnames!
error_cost2 <- matrix(c(0, 1, 4, 0), nrow = 2)

error_cost
error_cost2
```



```{r Decision Trees with cost matrix}
credit_cost <- C5.0(credit_train[-21], credit_train$default, costs = error_cost2)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, prop.r = FALSE, prop.c = FALSE, prop.chisq = FALSE, dnn = c('actual default', 'predicted default'))

```
Notice that the total accuracy is now 59% But the type of errors has varied and the more costly error has been reduced. 


C5.0 can create an initial tree model then decompose the tree structure into a set of mutually exclusive rules. These rules can then be pruned and modified into a smaller set of potentially overlapping rules. The rules can be created using the rules option:

```{r Rule based models}
credit_model_rules <- C5.0(credit_train[-21], credit_train$default, rules = TRUE)
credit_model_rules
```

```{r}
summary(credit_model_rules)
```

```{r ROC}
probs <- predict(credit_boost10, credit_test, type = "prob")
# plot ROC curve
ROC <- roc(predictor=probs[,1], 
               response=credit_test$default,
               levels=rev(levels(credit_test$default)))
plot(ROC)
ROC$auc
```

credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column

```{r}
probs <- predict(credit_boost10, credit_test, type = "prob")
# plot ROC curve
pred <- ROCR::prediction(probs[, 2], credit_test$default)
perf_dt_10 <- ROCR::performance(pred,  'tpr',  'fpr')
#plot(perf_dt_10) #complains about coersing s4 into numeric, so we did it manually
plot(perf_dt_10@x.values[[1]], perf_dt_10@y.values[[1]], xlab = perf_dt_10@x.name[[1]], ylab = perf_dt_10@y.name[[1]], type = "l")
ROCR::performance(pred, 'auc')
```


```{r}
data.frame(predicted=probs, actual=credit_test$default) %>% ggplot(data=., aes(x=predicted.No)) +
  geom_density(aes(fill=credit_test$default), alpha=0.5) +
  xlab('Predicted probability of NO') +
  scale_fill_discrete(name="Actual label") +
  theme(legend.position=c(0.8,0.8))
```
```{r}
credit_predict <- predict(credit_model, credit_test)

```


```{r}
probs_1 <- predict(credit_model, credit_test, type = "prob")
# plot ROC curve
pred_1 <- ROCR::prediction(probs_1[, 2], credit_test$default)
perf_dt_1 <- ROCR::performance(pred_1,  'tpr',  'fpr')
#plot(perf_dt_1)
plot(perf_dt_1@x.values[[1]], perf_dt_1@y.values[[1]],  xlab = perf_dt_10@x.name[[1]], ylab = perf_dt_10@y.name[[1]], type = "l" )
ROCR::performance(pred_1, 'auc')
```



```{r}
credit_cost_pred <- predict(credit_cost, credit_test)

```

Next we try tuning the the model with the cost, but the following code fails!
```
probs_cost <- predict(credit_cost, credit_test, type = "prob")  
pred_cost <- prediction(probs_cost[,2], credit_test$default)  
perf_dt_cost <- performance(pred_cost, measure = 'tpr', x.measure = 'fpr')  
plot(perf_dt_cost)  
performance(pred_cost, 'auc')  
```

From the manual we see that:
When the
cost
argument is used in the main function, class probabilities derived from the class
distribution in the terminal nodes may not be consistent with the final predicted class.   For this
reason, requesting class probabilities from a model using unequal costs will throw an error

This post suggests a fix: https://stackoverflow.com/questions/32633764/error-in-predict-when-using-c-50-with-costs-and-predict-with-type-prob-to-draw



```{r}
# plot ROC for each method
roc_dt_1   <- data.frame(fpr = unlist(perf_dt_1@x.values), tpr = unlist(perf_dt_1@y.values))
roc_dt_1$method <- "DT 1"
roc_dt_10 <- data.frame(fpr = unlist(perf_dt_10@x.values), tpr = unlist(perf_dt_10@y.values))
roc_dt_10$method <- "DT 10"
rbind(roc_dt_1, roc_dt_10) %>%
  ggplot(data = ., aes(x = fpr, y = tpr, linetype = method, color = method)) + 
  geom_line() +
  geom_abline(a = 1, b = 0, linetype = 2) +
  scale_x_continuous(labels = scales::percent, lim = c(0,1)) +
  scale_y_continuous(labels = scales::percent, lim = c(0,1)) +
  theme(legend.position = c(0.8,0.2), legend.title = element_blank())
```

Next we consider tuning the DT model. Based on the caret package
See link https://topepo.github.io/caret/available-models.html

```{r What parameters we can tune in DT}
modelLookup("C5.0")
```

We have 3 parameters to tune in C5.0 decision trees implementation:  
1. trials: an integer specifying the number of boosting iterations. A value of one indicates that a single model is used  
2. model from caret git hub page https://github.com/topepo/caret/blob/master/models/files/C5.0.R seems there are two value: rules, tree    
3. winnow: A logical: should predictor winnowing (i.e feature selection) be used?  

```{r Tuning 3 parameters with caret default, message=FALSE, warning=FALSE, paged.print=FALSE}
credit_classifier3 <- train(credit_train[-21], credit_train$default , method = "C5.0", verbose = FALSE)
# we can do our own grid 
#grid <- expand.grid( .winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )
#credit_classifier3<- train(credit_train[-21], credit_train$default , method = "C5.0", verbose = FALSE, tuneGrid = grid)

credit_classifier3
credit_test_pred3 <- predict(credit_classifier3, credit_test)
CrossTable(credit_test$default, credit_test_pred3, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column


```

```{r tuning using ROC}
ctrl <- trainControl(method = "cv",   
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     allowParallel = FALSE)
m_cv_ROC <- train(credit_train[-21], credit_train$default,
      method = "C5.0",
      metric = "ROC",
      trControl = ctrl)
```


```{r}

probs_cv_ROC <- predict(m_cv_ROC, credit_test, type = "prob")
# plot ROC curve
pred_cv_ROC <- ROCR::prediction(probs_cv_ROC[, 2], credit_test$default)
perf_dt_cv_ROC <- ROCR::performance(pred_cv_ROC,  'tpr',  'fpr')
#plot(perf_dt_1)
plot(perf_dt_cv_ROC@x.values[[1]], perf_dt_cv_ROC@y.values[[1]],  xlab = perf_dt_cv_ROC@x.name[[1]], ylab = perf_dt_cv_ROC@y.name[[1]], type = "l" )
ROCR::performance(pred_cv_ROC, 'auc')
```


```{r}
# plot ROC for each method
roc_dt_1   <- data.frame(fpr=unlist(perf_dt_1@x.values), tpr=unlist(perf_dt_1@y.values))
roc_dt_1$method <- "DT 1"
roc_dt_10 <- data.frame(fpr=unlist(perf_dt_10@x.values), tpr=unlist(perf_dt_10@y.values))
roc_dt_10$method <- "DT 10"
roc_dt_cv_ROC <- data.frame(fpr=unlist(perf_dt_cv_ROC@x.values), tpr=unlist(perf_dt_cv_ROC@y.values))
roc_dt_cv_ROC$method <- "DT CV ROC"

rbind(roc_dt_1, roc_dt_10, roc_dt_cv_ROC) %>%
  ggplot(data=., aes(x=fpr, y=tpr, linetype=method, color=method)) + 
  geom_line() +
  geom_abline(a=1, b=0, linetype=2) +
  scale_x_continuous(labels=scales::percent, lim=c(0,1)) +
  scale_y_continuous(labels=scales::percent, lim=c(0,1)) +
  theme(legend.position=c(0.8,0.2), legend.title=element_blank())
```

>References  
>https://cran.r-project.org/web/packages/C50/vignettes/C5.0.html  
>https://cran.r-project.org/web/packages/C50/C50.pdf  

