---
title: "Bank Loan classifier with Naive Bays"
author: "Mohammed Zakaria"
#output:
#  html_document:
#    df_print: paged
#    toc: yes
#   keep_md: true
#    always_allow_html: yes

  
#  html_notebook:
#    highlight: espresso
#    theme: readable
#    toc: yes
always_allow_html: yes # this is for wordcloud2 output
output: github_document
---



```{r libraries, message=FALSE, warning= TRUE}
library(gmodels) # for CrossTable
library(ggplot2)
#install.packages("C50") # for DT algorithm
library(C50)
```


Pulling the data: The cancer data is from Brett Lantz's "Machine Learning with R" a repo for the data is under this link: https://github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv and original data can be found under https://archive.ics.uci.edu/ml

```{r getting data}
credit <- read.csv(file="C:/Users/mkzak/Documents/GitHub/FunWithR/FunWithR/3_DT/Data/credit.csv", stringsAsFactors = FALSE)
str(credit)

summary(credit)
```

from str() we see that the target feature is actually numerical representing a categorical variable (default vs. no default) and we see that it has 1 for no default and 2 for default. we will label them to make it more readable
```{r convert target feature into factor}
credit$default <- factor(credit$default, labels = c('No', 'YES'))


```


```{r checking few interesting features}
table(credit$checking_balance)

table(credit$default)

```

We divide the data 90:10. WE cannot assume that the data is random. So let us do that

```{r Randomize sample}
set.seed(123)
train_sample <- sample(1000, 900) # get 900 randomly selected numbers, each between 0 and 1000
str(train_sample)
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample,]
```


```{r check if the target feature is equally represented}
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))

```

Close! So we can proceed.

```{r Train model with default parameters}
# remove the "default" feature since this is the target one
credit_model <- C5.0(credit_train[-21], credit_train$default)
credit_model
#summary(credit_model) # uncomment to see summary for all the trees used
```

Here, number of samples is the number of examples
number of predictors is the number of features used
tree size is how many decision the depth of the tree is

More details can be seen from the summary function
```{r}
summary(credit_model)
```

we understand a line like
checking_balance in {unknown,> 200 DM}: 1 (412/50)
by saying that if we checking balance was unknown, or larger than 200 DM, then we are in class one. (we have 412 examples that we got right, and 50 that we classified wrongly based on this rule)

```{r Predict}
credit_predict <- predict(credit_model, credit_test)
```


```{r}
CrossTable(credit_test$default, credit_predict, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column
```

From the table we can calculate the accuracy as .6 + .14 = 0.74. The model was particularly bad in missing 0.19 of the cases where there was a default. We can try improving the model using boosting

```{r modeling using boosting}
credit_boost10 <- C5.0(credit_train[-21], credit_train$default, trials = 10)
credit_boost10
```
Notice how the average tree size has schrunk! 

```{r}
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, prop.t = TRUE, dnn = c('actual defualt','predicted default')) # prop.c is for proportionaliy calculation per column
```

slight improvement accuracy is now 76% and the error rate is %17

Another approcah is to make one type of mistakes costier than the other
```{r Create a cost mistake}
matrix_dimentions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimentions) <- c("predicted", "actual")
matrix_dimentions


```

